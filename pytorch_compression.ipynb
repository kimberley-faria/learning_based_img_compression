{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83044b8-91bb-4a06-979f-0a8fbc9493a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "# import torch.multiprocessing as mp\n",
    "# from functools import partial\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from compressai.models import ScaleHyperprior\n",
    "from compressai.zoo import bmshj2018_hyperprior\n",
    "from torch import optim, nn, utils\n",
    "from torchvision import transforms\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE=32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f16ab4f-722f-41e3-84cb-9599637bf087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "# 1x1 convolution\n",
    "def conv1x1(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                     stride=stride, bias=False)\n",
    "\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv1x1(in_channels, in_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = conv3x3(in_channels, in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv3 = conv1x1(in_channels, out_channels, stride)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class cResnet39(pl.LightningModule):\n",
    "    def __init__(self, inchannels=192):\n",
    "        super().__init__()\n",
    "\n",
    "        # init a pretrained resnet\n",
    "        backbone = models.resnet50(weights=\"DEFAULT\")\n",
    "        layers = nn.ModuleList(list(backbone.children())[5:-1])\n",
    "\n",
    "        self.in_channels = inchannels\n",
    "        self.layer1_y_hat = self.make_layer(ResidualBlock, 128, 1)\n",
    "\n",
    "        self.in_channels = inchannels\n",
    "        self.layer1_scales_hat = self.make_layer(ResidualBlock, 128, 1)\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "\n",
    "        num_target_classes = 23\n",
    "        # self.classifier = nn.Linear(128*2048, 23)\n",
    "        self.classifier = nn.Linear(2048, 23)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.training_targets = []\n",
    "        self.validation_targets = []\n",
    "        \n",
    "        self.training_predictions = []\n",
    "        self.validation_predictions = []\n",
    "        \n",
    "        self.training_step_losses = []\n",
    "        self.validation_step_losses = []\n",
    "        \n",
    "        self.top1_accuracy = Accuracy(task=\"multiclass\", num_classes=num_target_classes)\n",
    "        self.top5_accuracy = Accuracy(task=\"multiclass\", num_classes=num_target_classes, top_k=5)\n",
    "        \n",
    "        # save hyper-parameters to self.hparamsm auto-logged by wandb\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = nn.ModuleList()\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        # print(*layers)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, y_hat, scales_hat):\n",
    "        y_hat = self.layer1_y_hat(y_hat.T)\n",
    "        scales_hat = self.layer1_scales_hat(scales_hat.T)\n",
    "\n",
    "        x = torch.concat((y_hat, scales_hat), 1)\n",
    "        \n",
    "        # self.feature_extractor.eval()\n",
    "        # with torch.no_grad():\n",
    "        representations = self.feature_extractor(x)\n",
    "\n",
    "        representations = representations.view(representations.size(0), -1)\n",
    "        \n",
    "        output = self.classifier(representations)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_hat, scales_hat, target = batch\n",
    "        target = target.to(device)\n",
    "\n",
    "        y_hat, scales_hat = y_hat.to(device), scales_hat.to(device)\n",
    "      \n",
    "        predict = self.forward(y_hat.T, scales_hat.T)\n",
    "        batch_loss = F.cross_entropy(predict, target)\n",
    "        \n",
    "        self.training_step_losses.append(batch_loss)\n",
    "\n",
    "        self.training_targets.append(target)\n",
    "        self.training_predictions.append(predict)\n",
    "        \n",
    "        return batch_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y_hat, scales_hat, target = batch\n",
    "        \n",
    "        target = target.to(device)\n",
    "\n",
    "        y_hat, scales_hat = y_hat.to(device), scales_hat.to(device)\n",
    "      \n",
    "        with torch.no_grad():\n",
    "            predict = self.forward(y_hat.T, scales_hat.T)\n",
    "            batch_loss = F.cross_entropy(predict, target)\n",
    "            \n",
    "        self.validation_step_losses.append(batch_loss)\n",
    "\n",
    "            \n",
    "        self.validation_targets.append(target)\n",
    "        self.validation_predictions.append(predict)\n",
    "        \n",
    "        return batch_loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # print(torch.cat(self.training_targets).shape)\n",
    "        # print(torch.cat(self.training_predictions).shape)\n",
    "        \n",
    "        loss = F.cross_entropy(torch.cat(self.training_predictions), torch.cat(self.training_targets))\n",
    "        # loss1 = sum(self.training_step_losses) / len(self.training_step_losses)\n",
    "        top1_accuracy = self.top1_accuracy(torch.cat(self.training_predictions), torch.cat(self.training_targets)) \n",
    "        top5_accuracy = self.top5_accuracy(torch.cat(self.training_predictions), torch.cat(self.training_targets)) \n",
    "        print(\"\\nTrain loss:\", loss)\n",
    "        print(\"Train top-1 acc:\", top1_accuracy)\n",
    "        print(\"Train top-5 acc:\", top5_accuracy)\n",
    "        self.log(\"train loss\", loss)\n",
    "        self.log(\"train top-1 acc\", top1_accuracy)\n",
    "        self.log(\"train top-5 acc\", top5_accuracy)\n",
    "        self.training_targets.clear()\n",
    "        self.training_predictions.clear()\n",
    "        self.training_step_losses.clear()\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        print(torch.cat(self.validation_targets).shape)\n",
    "        print(torch.cat(self.validation_predictions).shape)\n",
    "        \n",
    "        loss = F.cross_entropy(torch.cat(self.validation_predictions), torch.cat(self.validation_targets))\n",
    "        top1_accuracy = self.top1_accuracy(torch.cat(self.validation_predictions), torch.cat(self.validation_targets)) \n",
    "        top5_accuracy = self.top5_accuracy(torch.cat(self.validation_predictions), torch.cat(self.validation_targets)) \n",
    "        # loss = sum(self.validation_step_losses) / len(self.validation_step_losses)\n",
    "        \n",
    "        print(\"\\nVal loss:\", loss)\n",
    "        print(\"Val top-1 acc:\", top1_accuracy)\n",
    "        print(\"Val top-5 acc:\", top5_accuracy)\n",
    "        self.log(\"val loss\", loss)\n",
    "        self.log(\"val top-1 acc\", top1_accuracy)\n",
    "        self.log(\"val top-5 acc\", top5_accuracy)\n",
    "        self.validation_targets.clear()\n",
    "        self.validation_predictions.clear()\n",
    "        self.validation_step_losses.clear()\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "        # return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc4d41d-cc4b-488e-8269-3753032955cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_psnr(a, b):\n",
    "    mse = torch.mean((a - b)**2).item()\n",
    "    return -10 * math.log10(mse)\n",
    "\n",
    "def compute_msssim(a, b):\n",
    "    return ms_ssim(a, b, data_range=1.).item()\n",
    "\n",
    "def compute_bpp(out_net):\n",
    "    size = out_net['x_hat'].size()\n",
    "    num_pixels = size[0] * size[2] * size[3]\n",
    "    return sum(torch.log(likelihoods).sum() / (-math.log(2) * num_pixels)\n",
    "              for likelihoods in out_net['likelihoods'].values()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10f3758c-8991-415c-85ca-883e66b68e95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use the train-validation-test split\n",
    "# 1 provided in the dataset, with 2125 training images, 125\n",
    "# validation images and 250 testing images for each class.\n",
    "\n",
    "class MINCDataset(data.Dataset):\n",
    "    NUM_CLASS = 23\n",
    "\n",
    "    def __init__(self, root=os.path.expanduser('../'),\n",
    "                 train=True, quality=1):\n",
    "        split = 'train' if train == True else 'val'\n",
    "        root = os.path.join(root, 'minc-2500')\n",
    "        print(root)\n",
    "        \n",
    "        self.classes, self.class_to_idx = find_classes(root + '/images')\n",
    "        if split == 'train':\n",
    "            filename = os.path.join(root, 'labels/train1.txt') # 2125\n",
    "        else:\n",
    "            filename = os.path.join(root, 'labels/validate1.txt') # 125\n",
    "\n",
    "        self.images, self.crs, self.labels = make_dataset(filename, root, self.class_to_idx, quality)\n",
    "        \n",
    "        self.compression_model = bmshj2018_hyperprior(quality=quality, pretrained=True).eval().to(device)\n",
    "\n",
    "        assert (len(self.images) == len(self.labels))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _image = self.images[index]\n",
    "        _img = Image.open(_image).convert('RGB')\n",
    "        _label = self.labels[index]\n",
    "        \n",
    "        _img = transforms.ToTensor()(_img)\n",
    "        _img = transforms.Resize(384)(_img)\n",
    "    \n",
    "        _img = _img.unsqueeze(0).to(device)\n",
    "        \n",
    "        _compressed_rep = self.crs[index]\n",
    "\n",
    "        with open(_compressed_rep, 'rb') as handle:\n",
    "            compressed_rep = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _z_hat = self.compression_model.entropy_bottleneck.decompress(compressed_rep['strings'][1], compressed_rep['shape'])\n",
    "            _scales_hat = self.compression_model.h_s(_z_hat)\n",
    "            _indexes = self.compression_model.gaussian_conditional.build_indexes(_scales_hat)\n",
    "            _y_hat = self.compression_model.gaussian_conditional.decompress(compressed_rep['strings'][0], _indexes, _z_hat.dtype)\n",
    "        \n",
    "        _y_hat, _scales_hat = torch.squeeze(_y_hat, 0).to(device), torch.squeeze(_scales_hat, 0).to(device)\n",
    "        _y_hat = transforms.Resize(32)(_y_hat)\n",
    "        _scales_hat = transforms.Resize(32)(_scales_hat)\n",
    "        _y_hat = transforms.RandomCrop(28)(_y_hat)\n",
    "        _scales_hat = transforms.RandomCrop(28)(_scales_hat)\n",
    "        \n",
    "        p = float(torch.randint(0, 2, (1, )).item())\n",
    "        _y_hat = transforms.RandomHorizontalFlip(p=p)(_y_hat)\n",
    "        _scales_hat = transforms.RandomHorizontalFlip(p=p)(_scales_hat)\n",
    "\n",
    "        return _image, _y_hat, _scales_hat, _label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "def make_dataset(filename, datadir, class_to_idx, quality):\n",
    "    images = []\n",
    "    labels = []\n",
    "    crs = []\n",
    "    \n",
    "    i = 0\n",
    "    with open(os.path.join(filename), \"r\") as lines:\n",
    "        for line in lines:\n",
    "            _image = os.path.join(datadir, line.rstrip('\\n'))\n",
    "            _dirname = os.path.split(os.path.dirname(_image))[1]\n",
    "            _compressed_rep = os.path.join(datadir, 'compressed_rep', f'bpp{quality}', _dirname, os.path.splitext(os.path.split(_image)[1])[0])\n",
    "            assert os.path.isfile(_image)\n",
    "            assert os.path.isfile(_compressed_rep)\n",
    "            label = class_to_idx[_dirname]\n",
    "            images.append(_image)\n",
    "            crs.append(_compressed_rep)\n",
    "            labels.append(label)\n",
    "            \n",
    "            i += 1\n",
    "            if i % 1000 == 0: sys.stdout.write('\\r'+str(i)+' items loaded')\n",
    "            \n",
    "    sys.stdout.write('\\r'+str(i)+' items loaded')\n",
    "                           \n",
    "              \n",
    "    return images, crs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede08490-6801-44fe-869e-8124582fe8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../minc-2500\n",
      "48875 items loaded../minc-2500\n",
      "2875 items loaded"
     ]
    }
   ],
   "source": [
    "train_minc = MINCDataset(train=True,quality=8)\n",
    "val_minc = MINCDataset(train=False,quality=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d256413-5997-42b8-9aa7-9787c5577fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = utils.data.DataLoader(train_minc, batch_size=32, shuffle=True)\n",
    "valdn_loader = utils.data.DataLoader(val_minc, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a6bed55-37ca-464a-b6ce-242769a9b5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kfaria_umass_edu/.conda/envs/ds696-project/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | layer1_y_hat      | Sequential         | 1.1 M \n",
      "1 | layer1_scales_hat | Sequential         | 1.1 M \n",
      "2 | feature_extractor | Sequential         | 23.3 M\n",
      "3 | classifier        | Linear             | 47.1 K\n",
      "4 | top1_accuracy     | MulticlassAccuracy | 0     \n",
      "5 | top5_accuracy     | MulticlassAccuracy | 0     \n",
      "---------------------------------------------------------\n",
      "25.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.5 M    Total params\n",
      "102.181   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:03<00:00,  3.65s/it, v_num=]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 60.17it/s]\u001b[Atorch.Size([32])\n",
      "torch.Size([32, 23])\n",
      "\n",
      "Val loss: tensor(2.9953, device='cuda:0')\n",
      "Val top-1 acc: tensor(0.0625, device='cuda:0')\n",
      "Val top-5 acc: tensor(0.3125, device='cuda:0')\n",
      "Epoch 0: 100%|██████████| 1/1 [00:05<00:00,  5.31s/it, v_num=]\n",
      "Epoch 0: 100%|██████████| 1/1 [00:05<00:00,  5.32s/it, v_num=]        \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: tensor(3.1691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Train top-1 acc: tensor(0., device='cuda:0')\n",
      "Train top-5 acc: tensor(0.1875, device='cuda:0')\n",
      "Epoch 0: 100%|██████████| 1/1 [00:05<00:00,  5.33s/it, v_num=]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-river-122</strong> at: <a href='https://wandb.ai/umass-iesl-is/696ds-learning-based-image-compression/runs/tfpn4zat' target=\"_blank\">https://wandb.ai/umass-iesl-is/696ds-learning-based-image-compression/runs/tfpn4zat</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230410_174503-tfpn4zat/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project='696ds-learning-based-image-compression', log_model=True)\n",
    "\n",
    "downstream_model = cResnet39(inchannels=320)\n",
    "downstream_model = downstream_model.to(device)\n",
    "trainer = pl.Trainer(fast_dev_run = True, logger=wandb_logger)\n",
    "# print(0.001*48875 , 0.015*2875)\n",
    "# trainer = pl.Trainer(max_epochs=1, logger=wandb_logger)\n",
    "trainer.fit(model=downstream_model, train_dataloaders=train_loader, val_dataloaders=valdn_loader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9041d-99a0-4404-9eea-93d8991b006e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074467e1-800e-40a2-a0e4-0573f0c351e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481fa3c4-ed02-49cf-87e2-b5a34480e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_reference = \"umass-iesl-is/696ds-learning-based-image-compression/model-sh08q7s9:v0\"\n",
    "\n",
    "# download checkpoint locally (if not already cached)\n",
    "run = wandb.init(project='696ds-learning-based-image-compression')\n",
    "artifact = run.use_artifact(checkpoint_reference, type=\"model\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# load checkpoint\n",
    "downstream_model = cResnet39.load_from_checkpoint(os.path.join(artifact_dir,\"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc29d7a-0186-41f3-af92-e3794a04b015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "downstream_model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        x, y_hat, scales_hat, target = batch\n",
    "        \n",
    "        target = target.to(device)\n",
    "\n",
    "        y_hat, scales_hat = y_hat.to(device), scales_hat.to(device)\n",
    "\n",
    "        predict = downstream_model(y_hat.T, scales_hat.T)\n",
    "        loss = F.cross_entropy(predict, target)\n",
    "        print(\"top 1:\", Accuracy(task=\"multiclass\", num_classes=23).to(device)(predict, target).item())\n",
    "        print(\"top 5:\", Accuracy(task=\"multiclass\", num_classes=23, top_k=5).to(device)(predict, target).item())\n",
    "        print(\"loss :\", loss.item())\n",
    "        \n",
    "        for idx, img in enumerate(x):\n",
    "            print(\"Label\", target[idx].item())\n",
    "            \n",
    "            img = Image.open(img).convert('RGB')\n",
    "            \n",
    "            f, axarr = plt.subplots(1,3)\n",
    "            axarr[0].axis('off')\n",
    "            axarr[1].axis('off')\n",
    "            axarr[2].axis('off')\n",
    "            \n",
    "            axarr[0].imshow(img)\n",
    "            axarr[0].title.set_text('Image')\n",
    "            \n",
    "            axarr[1].imshow(y_hat[idx].sum(axis=0).cpu())\n",
    "            axarr[1].title.set_text('latent space')\n",
    "            \n",
    "            axarr[2].imshow(scales_hat[idx].sum(axis=0).cpu())\n",
    "            axarr[2].title.set_text('std. dev.')\n",
    "            \n",
    "            f.suptitle(f'Target: {train_minc.classes[target[idx].item()]}, Prediction: {train_minc.classes[torch.argmax(predict[idx])]}, Top 5: {[train_minc.classes[p] for p in torch.topk(predict[idx], 5).indices.tolist()]}', fontsize=10)\n",
    "            f.tight_layout()\n",
    "            f.subplots_adjust(top=1.3)\n",
    "            \n",
    "            \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a7788-9874-4516-92a9-680646677a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.conda-ds696-project)",
   "language": "python",
   "name": "conda-env-.conda-ds696-project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
