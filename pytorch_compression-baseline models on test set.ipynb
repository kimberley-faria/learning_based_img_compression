{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83044b8-91bb-4a06-979f-0a8fbc9493a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "# import torch.multiprocessing as mp\n",
    "# from functools import partial\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from compressai.models import ScaleHyperprior\n",
    "from compressai.zoo import bmshj2018_hyperprior\n",
    "from torch import optim, nn, utils\n",
    "from torchvision import transforms\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE=32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f16ab4f-722f-41e3-84cb-9599637bf087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "# 1x1 convolution\n",
    "def conv1x1(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                     stride=stride, bias=False)\n",
    "\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv1x1(in_channels, in_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = conv3x3(in_channels, in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv3 = conv1x1(in_channels, out_channels, stride)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class cResnet39(pl.LightningModule):\n",
    "    def __init__(self, inchannels=192):\n",
    "        super().__init__()\n",
    "\n",
    "        # init a pretrained resnet\n",
    "        backbone = models.resnet50(weights=\"DEFAULT\")\n",
    "        layers = nn.ModuleList(list(backbone.children())[5:-1])\n",
    "\n",
    "        self.in_channels = inchannels\n",
    "        self.layer1_y_hat = self.make_layer(ResidualBlock, 128, 1)\n",
    "\n",
    "        self.in_channels = inchannels\n",
    "        self.layer1_scales_hat = self.make_layer(ResidualBlock, 128, 1)\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "\n",
    "        num_target_classes = 23\n",
    "        # self.classifier = nn.Linear(128*2048, 23)\n",
    "        self.classifier = nn.Linear(2048, 23)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.training_targets = []\n",
    "        self.validation_targets = []\n",
    "        \n",
    "        self.training_predictions = []\n",
    "        self.validation_predictions = []\n",
    "        \n",
    "        self.training_step_losses = []\n",
    "        self.validation_step_losses = []\n",
    "        \n",
    "        self.top1_accuracy = Accuracy(task=\"multiclass\", num_classes=num_target_classes)\n",
    "        self.top5_accuracy = Accuracy(task=\"multiclass\", num_classes=num_target_classes, top_k=5)\n",
    "        \n",
    "        # save hyper-parameters to self.hparamsm auto-logged by wandb\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = nn.ModuleList()\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        # print(*layers)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, y_hat, scales_hat):\n",
    "        y_hat = self.layer1_y_hat(y_hat.T)\n",
    "        scales_hat = self.layer1_scales_hat(scales_hat.T)\n",
    "\n",
    "        x = torch.concat((y_hat, scales_hat), 1)\n",
    "        \n",
    "        # self.feature_extractor.eval()\n",
    "        # with torch.no_grad():\n",
    "        representations = self.feature_extractor(x)\n",
    "\n",
    "        representations = representations.view(representations.size(0), -1)\n",
    "        \n",
    "        output = self.classifier(representations)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_hat, scales_hat, target = batch\n",
    "        target = target.to(device)\n",
    "\n",
    "        y_hat, scales_hat = y_hat.to(device), scales_hat.to(device)\n",
    "      \n",
    "        predict = self.forward(y_hat.T, scales_hat.T)\n",
    "        batch_loss = F.cross_entropy(predict, target)\n",
    "        \n",
    "        self.training_step_losses.append(batch_loss)\n",
    "\n",
    "        self.training_targets.append(target)\n",
    "        self.training_predictions.append(predict)\n",
    "        \n",
    "        return batch_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y_hat, scales_hat, target = batch\n",
    "        \n",
    "        target = target.to(device)\n",
    "\n",
    "        y_hat, scales_hat = y_hat.to(device), scales_hat.to(device)\n",
    "      \n",
    "        with torch.no_grad():\n",
    "            predict = self.forward(y_hat.T, scales_hat.T)\n",
    "            batch_loss = F.cross_entropy(predict, target)\n",
    "            \n",
    "        self.validation_step_losses.append(batch_loss)\n",
    "\n",
    "            \n",
    "        self.validation_targets.append(target)\n",
    "        self.validation_predictions.append(predict)\n",
    "        \n",
    "        return batch_loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # print(torch.cat(self.training_targets).shape)\n",
    "        # print(torch.cat(self.training_predictions).shape)\n",
    "        \n",
    "        loss = F.cross_entropy(torch.cat(self.training_predictions), torch.cat(self.training_targets))\n",
    "        # loss1 = sum(self.training_step_losses) / len(self.training_step_losses)\n",
    "        top1_accuracy = self.top1_accuracy(torch.cat(self.training_predictions), torch.cat(self.training_targets)) \n",
    "        top5_accuracy = self.top5_accuracy(torch.cat(self.training_predictions), torch.cat(self.training_targets)) \n",
    "        print(\"\\nTrain loss:\", loss)\n",
    "        print(\"Train top-1 acc:\", top1_accuracy)\n",
    "        print(\"Train top-5 acc:\", top5_accuracy)\n",
    "        self.log(\"train loss\", loss)\n",
    "        self.log(\"train top-1 acc\", top1_accuracy)\n",
    "        self.log(\"train top-5 acc\", top5_accuracy)\n",
    "        self.training_targets.clear()\n",
    "        self.training_predictions.clear()\n",
    "        self.training_step_losses.clear()\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        print(torch.cat(self.validation_targets).shape)\n",
    "        print(torch.cat(self.validation_predictions).shape)\n",
    "        \n",
    "        loss = F.cross_entropy(torch.cat(self.validation_predictions), torch.cat(self.validation_targets))\n",
    "        top1_accuracy = self.top1_accuracy(torch.cat(self.validation_predictions), torch.cat(self.validation_targets)) \n",
    "        top5_accuracy = self.top5_accuracy(torch.cat(self.validation_predictions), torch.cat(self.validation_targets)) \n",
    "        # loss = sum(self.validation_step_losses) / len(self.validation_step_losses)\n",
    "        \n",
    "        print(\"\\nVal loss:\", loss)\n",
    "        print(\"Val top-1 acc:\", top1_accuracy)\n",
    "        print(\"Val top-5 acc:\", top5_accuracy)\n",
    "        self.log(\"val loss\", loss)\n",
    "        self.log(\"val top-1 acc\", top1_accuracy)\n",
    "        self.log(\"val top-5 acc\", top5_accuracy)\n",
    "        self.validation_targets.clear()\n",
    "        self.validation_predictions.clear()\n",
    "        self.validation_step_losses.clear()\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\"}]\n",
    "        \n",
    "        # return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc4d41d-cc4b-488e-8269-3753032955cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_psnr(a, b):\n",
    "    mse = torch.mean((a - b)**2).item()\n",
    "    return -10 * math.log10(mse)\n",
    "\n",
    "def compute_msssim(a, b):\n",
    "    return ms_ssim(a, b, data_range=1.).item()\n",
    "\n",
    "def compute_bpp(out_net):\n",
    "    size = out_net['x_hat'].size()\n",
    "    num_pixels = size[0] * size[2] * size[3]\n",
    "    return sum(torch.log(likelihoods).sum() / (-math.log(2) * num_pixels)\n",
    "              for likelihoods in out_net['likelihoods'].values()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10f3758c-8991-415c-85ca-883e66b68e95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use the train-validation-test split\n",
    "# 1 provided in the dataset, with 2125 training images, 125\n",
    "# validation images and 250 testing images for each class.\n",
    "\n",
    "class MINCDataset(data.Dataset):\n",
    "    NUM_CLASS = 23\n",
    "\n",
    "    def __init__(self, root=os.path.expanduser('../'),\n",
    "                 split = 'train', quality=1):\n",
    "        # split = one of ['train', 'val', 'test']\n",
    "        root = os.path.join(root, 'minc-2500')\n",
    "        print(root)\n",
    "        \n",
    "        self.classes, self.class_to_idx = find_classes(root + '/images')\n",
    "        if split == 'train':\n",
    "            filename = os.path.join(root, 'labels/train1.txt') # 2125\n",
    "        elif split == 'val':\n",
    "            filename = os.path.join(root, 'labels/validate1.txt') # 125\n",
    "        else:\n",
    "            filename = os.path.join(root, 'labels/test1.txt') # 250\n",
    "\n",
    "        self.images, self.crs, self.labels = make_dataset(filename, root, self.class_to_idx, quality)\n",
    "        \n",
    "        self.compression_model = bmshj2018_hyperprior(quality=quality, pretrained=True).eval().to(device)\n",
    "\n",
    "        assert (len(self.images) == len(self.labels))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _image = self.images[index]\n",
    "        _img = Image.open(_image).convert('RGB')\n",
    "        _label = self.labels[index]\n",
    "        \n",
    "        _img = transforms.ToTensor()(_img)\n",
    "        _img = transforms.Resize(384)(_img)\n",
    "    \n",
    "        _img = _img.unsqueeze(0).to(device)\n",
    "        \n",
    "        _compressed_rep = self.crs[index]\n",
    "\n",
    "        with open(_compressed_rep, 'rb') as handle:\n",
    "            compressed_rep = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _z_hat = self.compression_model.entropy_bottleneck.decompress(compressed_rep['strings'][1], compressed_rep['shape'])\n",
    "            _scales_hat = self.compression_model.h_s(_z_hat)\n",
    "            _indexes = self.compression_model.gaussian_conditional.build_indexes(_scales_hat)\n",
    "            _y_hat = self.compression_model.gaussian_conditional.decompress(compressed_rep['strings'][0], _indexes, _z_hat.dtype)\n",
    "        \n",
    "        _y_hat, _scales_hat = torch.squeeze(_y_hat, 0).to(device), torch.squeeze(_scales_hat, 0).to(device)\n",
    "        _y_hat = transforms.Resize(32)(_y_hat)\n",
    "        _scales_hat = transforms.Resize(32)(_scales_hat)\n",
    "        _y_hat = transforms.RandomCrop(28)(_y_hat)\n",
    "        _scales_hat = transforms.RandomCrop(28)(_scales_hat)\n",
    "        \n",
    "        p = float(torch.randint(0, 2, (1, )).item())\n",
    "        _y_hat = transforms.RandomHorizontalFlip(p=p)(_y_hat)\n",
    "        _scales_hat = transforms.RandomHorizontalFlip(p=p)(_scales_hat)\n",
    "\n",
    "        return _image, _y_hat, _scales_hat, _label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "def make_dataset(filename, datadir, class_to_idx, quality):\n",
    "    images = []\n",
    "    labels = []\n",
    "    crs = []\n",
    "    \n",
    "    i = 0\n",
    "    with open(os.path.join(filename), \"r\") as lines:\n",
    "        for line in lines:\n",
    "            _image = os.path.join(datadir, line.rstrip('\\n'))\n",
    "            _dirname = os.path.split(os.path.dirname(_image))[1]\n",
    "            _compressed_rep = os.path.join(datadir, 'compressed_rep', f'bpp{quality}', _dirname, os.path.splitext(os.path.split(_image)[1])[0])\n",
    "            assert os.path.isfile(_image)\n",
    "            assert os.path.isfile(_compressed_rep)\n",
    "            label = class_to_idx[_dirname]\n",
    "            images.append(_image)\n",
    "            crs.append(_compressed_rep)\n",
    "            labels.append(label)\n",
    "            \n",
    "            i += 1\n",
    "            if i % 1000 == 0: sys.stdout.write('\\r'+str(i)+' items loaded')\n",
    "            \n",
    "    sys.stdout.write('\\r'+str(i)+' items loaded')\n",
    "                           \n",
    "              \n",
    "    return images, crs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede08490-6801-44fe-869e-8124582fe8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_minc = MINCDataset(train=True,quality=8)\n",
    "# val_minc = MINCDataset(train=False,quality=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d256413-5997-42b8-9aa7-9787c5577fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_loader = utils.data.DataLoader(train_minc, batch_size=32, shuffle=True)\n",
    "# valdn_loader = utils.data.DataLoader(val_minc, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a6bed55-37ca-464a-b6ce-242769a9b5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wandb_logger = WandbLogger(project='696ds-learning-based-image-compression', log_model=True)\n",
    "\n",
    "# downstream_model = cResnet39(inchannels=320)\n",
    "# downstream_model = downstream_model.to(device)\n",
    "# trainer = pl.Trainer(fast_dev_run = True, logger=wandb_logger)\n",
    "# # print(0.001*48875 , 0.015*2875)\n",
    "# # trainer = pl.Trainer(max_epochs=1, logger=wandb_logger)\n",
    "# trainer.fit(model=downstream_model, train_dataloaders=train_loader, val_dataloaders=valdn_loader)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f9041d-99a0-4404-9eea-93d8991b006e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../minc-2500\n",
      "5750 items loaded"
     ]
    }
   ],
   "source": [
    "test_minc = MINCDataset(split='test', quality=8)\n",
    "test_loader = utils.data.DataLoader(test_minc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "074467e1-800e-40a2-a0e4-0573f0c351e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "481fa3c4-ed02-49cf-87e2-b5a34480e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkfaria\u001b[0m (\u001b[33mumass-iesl-is\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kfaria_umass_edu/texture_recogn_reimpl/wandb/run-20230411_134415-rormd8nw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/umass-iesl-is/696ds-learning-based-image-compression/runs/rormd8nw' target=\"_blank\">playful-dawn-133</a></strong> to <a href='https://wandb.ai/umass-iesl-is/696ds-learning-based-image-compression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/umass-iesl-is/696ds-learning-based-image-compression' target=\"_blank\">https://wandb.ai/umass-iesl-is/696ds-learning-based-image-compression</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/umass-iesl-is/696ds-learning-based-image-compression/runs/rormd8nw' target=\"_blank\">https://wandb.ai/umass-iesl-is/696ds-learning-based-image-compression/runs/rormd8nw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-v4cfeyrj:v0, 195.25MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.5\n"
     ]
    }
   ],
   "source": [
    "checkpoint_reference = \"umass-iesl-is/696ds-learning-based-image-compression/model-v4cfeyrj:v0\"\n",
    "\n",
    "# download checkpoint locally (if not already cached)\n",
    "run = wandb.init(project='696ds-learning-based-image-compression')\n",
    "artifact = run.use_artifact(checkpoint_reference, type=\"model\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# load checkpoint\n",
    "downstream_model = cResnet39.load_from_checkpoint(os.path.join(artifact_dir,\"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fc29d7a-0186-41f3-af92-e3794a04b015",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kfaria_umass_edu/.conda/envs/ds696-project/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/scratch/gypsum-gpu055/6819722/ipykernel_47122/3009165379.py:14: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3571.)\n",
      "  predict = downstream_model(y_hat.T, scales_hat.T)\n"
     ]
    }
   ],
   "source": [
    "downstream_model.to(device).eval()\n",
    "\n",
    "predicts = []\n",
    "targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        x, y_hat, scales_hat, target = batch\n",
    "        \n",
    "        target = target.to(device)\n",
    "\n",
    "        y_hat, scales_hat = y_hat.to(device), scales_hat.to(device)\n",
    "\n",
    "        predict = downstream_model(y_hat.T, scales_hat.T)\n",
    "        \n",
    "        predicts.append(predict)\n",
    "        targets.append(target)\n",
    "        # loss = F.cross_entropy(predict, target)\n",
    "        \n",
    "        # print(\"loss :\", loss.item())\n",
    "        \n",
    "#         for idx, img in enumerate(x):\n",
    "#             print(\"Label\", target[idx].item())\n",
    "            \n",
    "#             img = Image.open(img).convert('RGB')\n",
    "            \n",
    "#             f, axarr = plt.subplots(1,3)\n",
    "#             axarr[0].axis('off')\n",
    "#             axarr[1].axis('off')\n",
    "#             axarr[2].axis('off')\n",
    "            \n",
    "#             axarr[0].imshow(img)\n",
    "#             axarr[0].title.set_text('Image')\n",
    "            \n",
    "#             axarr[1].imshow(y_hat[idx].sum(axis=0).cpu())\n",
    "#             axarr[1].title.set_text('latent space')\n",
    "            \n",
    "#             axarr[2].imshow(scales_hat[idx].sum(axis=0).cpu())\n",
    "#             axarr[2].title.set_text('std. dev.')\n",
    "            \n",
    "#             f.suptitle(f'Target: {test_minc.classes[target[idx].item()]}, Prediction: {test_minc.classes[torch.argmax(predict[idx])]}, Top 5: {[test_minc.classes[p] for p in torch.topk(predict[idx], 5).indices.tolist()]}', fontsize=10)\n",
    "#             f.tight_layout()\n",
    "#             f.subplots_adjust(top=1.3)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d185219-1a34-48d2-a31a-065a36061e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5750]), torch.Size([5750, 23]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.stack(targets).reshape(-1)\n",
    "p = torch.stack(predicts).reshape(-1, 23)\n",
    "t.shape, p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f7c84-beac-4575-a062-f01203dcfc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "412a7788-9874-4516-92a9-680646677a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test, quality 4\n",
      "top 1: 0.6179130673408508\n",
      "top 5: 0.8968695402145386\n"
     ]
    }
   ],
   "source": [
    "print(\"Test, quality 4\")\n",
    "print(\"top 1:\", Accuracy(task=\"multiclass\", num_classes=23).to(device)(p, t).item())\n",
    "print(\"top 5:\", Accuracy(task=\"multiclass\", num_classes=23, top_k=5).to(device)(p, t).item())         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3443f83-c70f-428c-95d7-46b2f26d0533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../minc-2500\n",
      "2875 items loaded"
     ]
    }
   ],
   "source": [
    "val_minc = MINCDataset(split='val', quality=8)\n",
    "val_loader = utils.data.DataLoader(val_minc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48ff5af3-3dc7-4f78-86e9-5fec531a4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(val_loader):\n",
    "        x, y_hat, scales_hat, target = batch\n",
    "        \n",
    "        target = target.to(device)\n",
    "\n",
    "        y_hat, scales_hat = y_hat.to(device), scales_hat.to(device)\n",
    "\n",
    "        predict = downstream_model(y_hat.T, scales_hat.T)\n",
    "        \n",
    "        predicts.append(predict)\n",
    "        targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0bef29c-987d-49d2-869e-3d3bdb203288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2875]), torch.Size([2875, 23]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.stack(targets).reshape(-1)\n",
    "p = torch.stack(predicts).reshape(-1, 23)\n",
    "t.shape, p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16b6ceb4-0b07-403e-8c79-b50a99746398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val, quality 4\n",
      "top 1: 0.61356520652771\n",
      "top 5: 0.8980869650840759\n"
     ]
    }
   ],
   "source": [
    "print(\"Val, quality 4\")\n",
    "print(\"top 1:\", Accuracy(task=\"multiclass\", num_classes=23).to(device)(p, t).item())\n",
    "print(\"top 5:\", Accuracy(task=\"multiclass\", num_classes=23, top_k=5).to(device)(p, t).item())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2121fcea-2f13-415f-b288-b1a688366779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-dawn-133</strong> at: <a href='https://wandb.ai/umass-iesl-is/696ds-learning-based-image-compression/runs/rormd8nw' target=\"_blank\">https://wandb.ai/umass-iesl-is/696ds-learning-based-image-compression/runs/rormd8nw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230411_134415-rormd8nw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.conda-ds696-project)",
   "language": "python",
   "name": "conda-env-.conda-ds696-project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
